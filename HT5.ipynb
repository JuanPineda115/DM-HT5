{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Martín Amado - 19020\n",
    "Juan Pablo Pineda - 19087\n",
    "Laura Tamath - 19365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOJA DE TRABAJO 5\n",
    "## Bayes Ingenuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import statsmodels.stats.diagnostic as diag\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.preprocessing\n",
    "import random\n",
    "import graphviz\n",
    "## import pyclustertend \n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.cm as cm\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Conjuntos de entrenamiento y prueba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./train.csv', encoding = \"latin1\")\n",
    "file = open('cuantitativas.txt', 'r')\n",
    "quant= file.read().splitlines()\n",
    "file = open('cualitativas.txt', 'r')\n",
    "quali= file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de interes\n",
    "print('\\033[36m' + 'Kurtosis: %f' % data['SalePrice'].kurt())\n",
    "print('\\033[36m' + 'Asimetría: %f' % data['SalePrice'].skew())\n",
    "data['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat,p = stats.shapiro(data[[\"SalePrice\"]].dropna())\n",
    "print('Prueba de Kolmogorov-Smirnov:\\np=%f\\n'% p)\n",
    "ks_statistic, p_value = diag.lilliefors(data[[\"SalePrice\"]].dropna())\n",
    "print('Prueba de Lilliefors:\\nks=%f\\np=%f'%(ks_statistic,p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data['SalePrice'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0)\r\n",
    "minPrice = data['SalePrice'].min()\r\n",
    "maxPrice = data['SalePrice'].max()\r\n",
    "divs = (maxPrice - minPrice) / 3\r\n",
    "data['priceRange'] = data['LotArea']\r\n",
    "\r\n",
    "data['priceRange'][data['SalePrice'] < minPrice + divs] = 0.0 #Economico\r\n",
    "data['priceRange'][data['SalePrice'] >= minPrice + divs] = 1.0 #Precio medio\r\n",
    "data['priceRange'][data['SalePrice'] >= minPrice + divs * 2] = 2.0 #Caro\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['priceRange']\r\n",
    "X = data.drop(['priceRange'], axis=1)\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Elabore un modelo de bayes ingenuo (naive bayes)\n",
    "- Laura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = GaussianNB()\r\n",
    "xtrain = X_train.apply(pd.to_numeric, errors='coerce')\r\n",
    "ytrain = y_train.apply(pd.to_numeric, errors='coerce')\r\n",
    "xtest = X_test.apply(pd.to_numeric, errors='coerce')\r\n",
    "xtrain.fillna(0, inplace=True)\r\n",
    "ytrain.fillna(0, inplace=True)\r\n",
    "xtest.fillna(0, inplace=True)\r\n",
    "gaussian.fit(xtrain,ytrain)\r\n",
    "model = gaussian.fit(xtrain,ytrain)\r\n",
    "y_pred = gaussian.predict(xtest)\r\n",
    "confusion = confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. use la variable categórica que hizo con el precio de las casas (barata, media y cara) como variable respuesta. \n",
    "- Laura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = list(y_train)\r\n",
    "print('Low:', pred.count(0.0))\r\n",
    "print('Medium:', pred.count(1.0))\r\n",
    "print('Expensive:', pred.count(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Utilice  el  modelo  con  el  conjunto  de  prueba  y  determine  la  eficiencia  del  algoritmo  para clasificar. -Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, y_pred)\r\n",
    "precision = metrics.precision_score(y_test, y_pred, average='micro')\r\n",
    "recall = metrics.recall_score(y_test, y_pred, average='micro')\r\n",
    "f1 = metrics.f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Haga un análisis de la eficiencia del  algoritmo usando una matriz de confusión. -Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Analisis de matriz de confucion\\n', confusion)\r\n",
    "print('Accuracy', accuracy)\r\n",
    "print('Precision', precision)\r\n",
    "print('Recall', recall)\r\n",
    "print('f1', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Analice el modelo. Explique si hay sobreajuste (overfitting) o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Haga  un  modelo  usando  validación  cruzada,  compare  los  resultados  de  este  con  los  del modelo anterior. ¿Cuál funcionó mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Compare la eficiencia del algoritmo con el resultado obtenido con el árbol de decisión (el de clasificación). ¿Cuál es mejor para predecir? ¿Cuál se demoró más en procesar? -Martin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de Naive Bayes nos resultó siendo más eficiente. Esto se debe principalmente a que la aplicación del algoritmo se hizo de manera correcta. A pesar de los errores que se tenían con el árbol de decisión, tuvo un menor tiempo de procesamiento promedio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1b1e0c645787a43952d50d550edd6247296232eaf059a1af998729e9d11faa38"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}